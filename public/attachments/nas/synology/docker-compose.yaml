# ==================================================================
# WARNING: This file is auto-generated by generate_docker_compose
# Do not modify this file directly. Instead, update the .env.example
# or docker-compose-template.yaml and regenerate this file.
# ==================================================================

x-shared-env: &shared-api-worker-env
  CONSOLE_API_URL: ''
  CONSOLE_WEB_URL: ''
  SERVICE_API_URL: ''
  APP_API_URL: ''
  APP_WEB_URL: ''
  FILES_URL: ''
  LOG_LEVEL: INFO
  LOG_FILE: /app/logs/server.log
  LOG_FILE_MAX_SIZE: 20
  LOG_FILE_BACKUP_COUNT: 5
  LOG_DATEFORMAT: '%Y-%m-%d %H:%M:%S'
  LOG_TZ: UTC
  DEBUG: false
  FLASK_DEBUG: false
  SECRET_KEY: sk-9f73s3ljTXVcMT3Blb3ljTqtsKiGHXVcMT3BlbkFJLK7U
  INIT_PASSWORD: ''
  DEPLOY_ENV: PRODUCTION
  CHECK_UPDATE_URL: 'https://updates.dify.ai'
  OPENAI_API_BASE: 'https://api.openai.com/v1'
  MIGRATION_ENABLED: true
  FILES_ACCESS_TIMEOUT: 300
  ACCESS_TOKEN_EXPIRE_MINUTES: 60
  REFRESH_TOKEN_EXPIRE_DAYS: 30
  APP_MAX_ACTIVE_REQUESTS: 0
  APP_MAX_EXECUTION_TIME: 1200
  DIFY_BIND_ADDRESS: 0.0.0.0
  DIFY_PORT: 5001
  SERVER_WORKER_AMOUNT: 1
  SERVER_WORKER_CLASS: gevent
  SERVER_WORKER_CONNECTIONS: 10
  CELERY_WORKER_CLASS: ''
  GUNICORN_TIMEOUT: 360
  CELERY_WORKER_AMOUNT: ''
  CELERY_AUTO_SCALE: false
  CELERY_MAX_WORKERS: ''
  CELERY_MIN_WORKERS: ''
  API_TOOL_DEFAULT_CONNECT_TIMEOUT: 10
  API_TOOL_DEFAULT_READ_TIMEOUT: 60
  DB_USERNAME: postgres
  DB_PASSWORD: difyai123456
  DB_HOST: db
  DB_PORT: 5432
  DB_DATABASE: dify
  SQLALCHEMY_POOL_SIZE: 30
  SQLALCHEMY_POOL_RECYCLE: 3600
  SQLALCHEMY_ECHO: false
  POSTGRES_MAX_CONNECTIONS: 100
  POSTGRES_SHARED_BUFFERS: 128MB
  POSTGRES_WORK_MEM: 4MB
  POSTGRES_MAINTENANCE_WORK_MEM: 64MB
  POSTGRES_EFFECTIVE_CACHE_SIZE: 4096MB
  REDIS_HOST: redis
  REDIS_PORT: 6379
  REDIS_USERNAME: ''
  REDIS_PASSWORD: difyai123456
  REDIS_USE_SSL: false
  REDIS_DB: 0
  REDIS_USE_SENTINEL: false
  REDIS_SENTINELS: ''
  REDIS_SENTINEL_SERVICE_NAME: ''
  REDIS_SENTINEL_USERNAME: ''
  REDIS_SENTINEL_PASSWORD: ''
  REDIS_SENTINEL_SOCKET_TIMEOUT: 0.1
  REDIS_USE_CLUSTERS: false
  REDIS_CLUSTERS: ''
  REDIS_CLUSTERS_PASSWORD: ''
  CELERY_BROKER_URL: 'redis://:difyai123456@redis:6379/1'
  BROKER_USE_SSL: false
  CELERY_USE_SENTINEL: false
  CELERY_SENTINEL_MASTER_NAME: ''
  CELERY_SENTINEL_SOCKET_TIMEOUT: 0.1
  WEB_API_CORS_ALLOW_ORIGINS: '*'
  CONSOLE_CORS_ALLOW_ORIGINS: '*'
  STORAGE_TYPE: opendal
  OPENDAL_SCHEME: fs
  OPENDAL_FS_ROOT: storage
  S3_ENDPOINT: ''
  S3_REGION: us-east-1
  S3_BUCKET_NAME: difyai
  S3_ACCESS_KEY: ''
  S3_SECRET_KEY: ''
  S3_USE_AWS_MANAGED_IAM: false
  AZURE_BLOB_ACCOUNT_NAME: difyai
  AZURE_BLOB_ACCOUNT_KEY: difyai
  AZURE_BLOB_CONTAINER_NAME: difyai-container
  AZURE_BLOB_ACCOUNT_URL: 'https://<your_account_name>.blob.core.windows.net'
  GOOGLE_STORAGE_BUCKET_NAME: your-bucket-name
  GOOGLE_STORAGE_SERVICE_ACCOUNT_JSON_BASE64: ''
  ALIYUN_OSS_BUCKET_NAME: your-bucket-name
  ALIYUN_OSS_ACCESS_KEY: your-access-key
  ALIYUN_OSS_SECRET_KEY: your-secret-key
  ALIYUN_OSS_ENDPOINT: 'https://oss-ap-southeast-1-internal.aliyuncs.com'
  ALIYUN_OSS_REGION: ap-southeast-1
  ALIYUN_OSS_AUTH_VERSION: v4
  ALIYUN_OSS_PATH: your-path
  TENCENT_COS_BUCKET_NAME: your-bucket-name
  TENCENT_COS_SECRET_KEY: your-secret-key
  TENCENT_COS_SECRET_ID: your-secret-id
  TENCENT_COS_REGION: your-region
  TENCENT_COS_SCHEME: your-scheme
  OCI_ENDPOINT: 'https://your-object-storage-namespace.compat.objectstorage.us-ashburn-1.oraclecloud.com'
  OCI_BUCKET_NAME: your-bucket-name
  OCI_ACCESS_KEY: your-access-key
  OCI_SECRET_KEY: your-secret-key
  OCI_REGION: us-ashburn-1
  HUAWEI_OBS_BUCKET_NAME: your-bucket-name
  HUAWEI_OBS_SECRET_KEY: your-secret-key
  HUAWEI_OBS_ACCESS_KEY: your-access-key
  HUAWEI_OBS_SERVER: your-server-url
  VOLCENGINE_TOS_BUCKET_NAME: your-bucket-name
  VOLCENGINE_TOS_SECRET_KEY: your-secret-key
  VOLCENGINE_TOS_ACCESS_KEY: your-access-key
  VOLCENGINE_TOS_ENDPOINT: your-server-url
  VOLCENGINE_TOS_REGION: your-region
  BAIDU_OBS_BUCKET_NAME: your-bucket-name
  BAIDU_OBS_SECRET_KEY: your-secret-key
  BAIDU_OBS_ACCESS_KEY: your-access-key
  BAIDU_OBS_ENDPOINT: your-server-url
  SUPABASE_BUCKET_NAME: your-bucket-name
  SUPABASE_API_KEY: your-access-key
  SUPABASE_URL: your-server-url
  VECTOR_STORE: weaviate
  WEAVIATE_ENDPOINT: 'http://weaviate:8080'
  WEAVIATE_API_KEY: WVF5YThaHlkYwhGUSmCRgsX3tD5ngdN8pkih
  QDRANT_URL: 'http://qdrant:6333'
  QDRANT_API_KEY: difyai123456
  QDRANT_CLIENT_TIMEOUT: 20
  QDRANT_GRPC_ENABLED: false
  QDRANT_GRPC_PORT: 6334
  MILVUS_URI: 'http://host.docker.internal:19530'
  MILVUS_TOKEN: ''
  MILVUS_USER: ''
  MILVUS_PASSWORD: ''
  MILVUS_ENABLE_HYBRID_SEARCH: False
  MYSCALE_HOST: myscale
  MYSCALE_PORT: 8123
  MYSCALE_USER: default
  MYSCALE_PASSWORD: ''
  MYSCALE_DATABASE: dify
  MYSCALE_FTS_PARAMS: ''
  COUCHBASE_CONNECTION_STRING: 'couchbase://couchbase-server'
  COUCHBASE_USER: Administrator
  COUCHBASE_PASSWORD: password
  COUCHBASE_BUCKET_NAME: Embeddings
  COUCHBASE_SCOPE_NAME: _default
  PGVECTOR_HOST: pgvector
  PGVECTOR_PORT: 5432
  PGVECTOR_USER: postgres
  PGVECTOR_PASSWORD: difyai123456
  PGVECTOR_DATABASE: dify
  PGVECTOR_MIN_CONNECTION: 1
  PGVECTOR_MAX_CONNECTION: 5
  PGVECTOR_PG_BIGM: false
  PGVECTOR_PG_BIGM_VERSION: 1.2-20240606
  PGVECTO_RS_HOST: pgvecto-rs
  PGVECTO_RS_PORT: 5432
  PGVECTO_RS_USER: postgres
  PGVECTO_RS_PASSWORD: difyai123456
  PGVECTO_RS_DATABASE: dify
  ANALYTICDB_KEY_ID: your-ak
  ANALYTICDB_KEY_SECRET: your-sk
  ANALYTICDB_REGION_ID: cn-hangzhou
  ANALYTICDB_INSTANCE_ID: gp-ab123456
  ANALYTICDB_ACCOUNT: testaccount
  ANALYTICDB_PASSWORD: testpassword
  ANALYTICDB_NAMESPACE: dify
  ANALYTICDB_NAMESPACE_PASSWORD: difypassword
  ANALYTICDB_HOST: gp-test.aliyuncs.com
  ANALYTICDB_PORT: 5432
  ANALYTICDB_MIN_CONNECTION: 1
  ANALYTICDB_MAX_CONNECTION: 5
  TIDB_VECTOR_HOST: tidb
  TIDB_VECTOR_PORT: 4000
  TIDB_VECTOR_USER: ''
  TIDB_VECTOR_PASSWORD: ''
  TIDB_VECTOR_DATABASE: dify
  TIDB_ON_QDRANT_URL: 'http://127.0.0.1'
  TIDB_ON_QDRANT_API_KEY: dify
  TIDB_ON_QDRANT_CLIENT_TIMEOUT: 20
  TIDB_ON_QDRANT_GRPC_ENABLED: false
  TIDB_ON_QDRANT_GRPC_PORT: 6334
  TIDB_PUBLIC_KEY: dify
  TIDB_PRIVATE_KEY: dify
  TIDB_API_URL: 'http://127.0.0.1'
  TIDB_IAM_API_URL: 'http://127.0.0.1'
  TIDB_REGION: regions/aws-us-east-1
  TIDB_PROJECT_ID: dify
  TIDB_SPEND_LIMIT: 100
  CHROMA_HOST: 127.0.0.1
  CHROMA_PORT: 8000
  CHROMA_TENANT: default_tenant
  CHROMA_DATABASE: default_database
  CHROMA_AUTH_PROVIDER: chromadb.auth.token_authn.TokenAuthClientProvider
  CHROMA_AUTH_CREDENTIALS: ''
  ORACLE_USER: dify
  ORACLE_PASSWORD: dify
  ORACLE_DSN: 'oracle:1521/FREEPDB1'
  ORACLE_CONFIG_DIR: /app/api/storage/wallet
  ORACLE_WALLET_LOCATION: /app/api/storage/wallet
  ORACLE_WALLET_PASSWORD: dify
  ORACLE_IS_AUTONOMOUS: false
  RELYT_HOST: db
  RELYT_PORT: 5432
  RELYT_USER: postgres
  RELYT_PASSWORD: difyai123456
  RELYT_DATABASE: postgres
  OPENSEARCH_HOST: opensearch
  OPENSEARCH_PORT: 9200
  OPENSEARCH_USER: admin
  OPENSEARCH_PASSWORD: admin
  OPENSEARCH_SECURE: true
  TENCENT_VECTOR_DB_URL: 'http://127.0.0.1'
  TENCENT_VECTOR_DB_API_KEY: dify
  TENCENT_VECTOR_DB_TIMEOUT: 30
  TENCENT_VECTOR_DB_USERNAME: dify
  TENCENT_VECTOR_DB_DATABASE: dify
  TENCENT_VECTOR_DB_SHARD: 1
  TENCENT_VECTOR_DB_REPLICAS: 2
  ELASTICSEARCH_HOST: 0.0.0.0
  ELASTICSEARCH_PORT: 9200
  ELASTICSEARCH_USERNAME: elastic
  ELASTICSEARCH_PASSWORD: elastic
  KIBANA_PORT: 5601
  BAIDU_VECTOR_DB_ENDPOINT: 'http://127.0.0.1:5287'
  BAIDU_VECTOR_DB_CONNECTION_TIMEOUT_MS: 30000
  BAIDU_VECTOR_DB_ACCOUNT: root
  BAIDU_VECTOR_DB_API_KEY: dify
  BAIDU_VECTOR_DB_DATABASE: dify
  BAIDU_VECTOR_DB_SHARD: 1
  BAIDU_VECTOR_DB_REPLICAS: 3
  VIKINGDB_ACCESS_KEY: your-ak
  VIKINGDB_SECRET_KEY: your-sk
  VIKINGDB_REGION: cn-shanghai
  VIKINGDB_HOST: api-vikingdb.xxx.volces.com
  VIKINGDB_SCHEMA: http
  VIKINGDB_CONNECTION_TIMEOUT: 30
  VIKINGDB_SOCKET_TIMEOUT: 30
  LINDORM_URL: 'http://lindorm:30070'
  LINDORM_USERNAME: lindorm
  LINDORM_PASSWORD: lindorm
  OCEANBASE_VECTOR_HOST: oceanbase
  OCEANBASE_VECTOR_PORT: 2881
  OCEANBASE_VECTOR_USER: root@test
  OCEANBASE_VECTOR_PASSWORD: difyai123456
  OCEANBASE_VECTOR_DATABASE: test
  OCEANBASE_CLUSTER_NAME: difyai
  OCEANBASE_MEMORY_LIMIT: 6G
  OPENGAUSS_HOST: opengauss
  OPENGAUSS_PORT: 6600
  OPENGAUSS_USER: postgres
  OPENGAUSS_PASSWORD: Dify@123
  OPENGAUSS_DATABASE: dify
  OPENGAUSS_MIN_CONNECTION: 1
  OPENGAUSS_MAX_CONNECTION: 5
  OPENGAUSS_ENABLE_PQ: false
  UPSTASH_VECTOR_URL: 'https://xxx-vector.upstash.io'
  UPSTASH_VECTOR_TOKEN: dify
  UPLOAD_FILE_SIZE_LIMIT: 15
  UPLOAD_FILE_BATCH_LIMIT: 5
  ETL_TYPE: dify
  UNSTRUCTURED_API_URL: ''
  UNSTRUCTURED_API_KEY: ''
  SCARF_NO_ANALYTICS: true
  PROMPT_GENERATION_MAX_TOKENS: 512
  CODE_GENERATION_MAX_TOKENS: 1024
  MULTIMODAL_SEND_FORMAT: base64
  UPLOAD_IMAGE_FILE_SIZE_LIMIT: 10
  UPLOAD_VIDEO_FILE_SIZE_LIMIT: 100
  UPLOAD_AUDIO_FILE_SIZE_LIMIT: 50
  SENTRY_DSN: ''
  API_SENTRY_DSN: ''
  API_SENTRY_TRACES_SAMPLE_RATE: 1.0
  API_SENTRY_PROFILES_SAMPLE_RATE: 1.0
  WEB_SENTRY_DSN: ''
  NOTION_INTEGRATION_TYPE: public
  NOTION_CLIENT_SECRET: ''
  NOTION_CLIENT_ID: ''
  NOTION_INTERNAL_SECRET: ''
  MAIL_TYPE: resend
  MAIL_DEFAULT_SEND_FROM: ''
  RESEND_API_URL: 'https://api.resend.com'
  RESEND_API_KEY: your-resend-api-key
  SMTP_SERVER: ''
  SMTP_PORT: 465
  SMTP_USERNAME: ''
  SMTP_PASSWORD: ''
  SMTP_USE_TLS: true
  SMTP_OPPORTUNISTIC_TLS: false
  INDEXING_MAX_SEGMENTATION_TOKENS_LENGTH: 4000
  INVITE_EXPIRY_HOURS: 72
  RESET_PASSWORD_TOKEN_EXPIRY_MINUTES: 5
  CODE_EXECUTION_ENDPOINT: 'http://sandbox:8194'
  CODE_EXECUTION_API_KEY: dify-sandbox
  CODE_MAX_NUMBER: 9223372036854775807
  CODE_MIN_NUMBER: -9223372036854775808
  CODE_MAX_DEPTH: 5
  CODE_MAX_PRECISION: 20
  CODE_MAX_STRING_LENGTH: 80000
  CODE_MAX_STRING_ARRAY_LENGTH: 30
  CODE_MAX_OBJECT_ARRAY_LENGTH: 30
  CODE_MAX_NUMBER_ARRAY_LENGTH: 1000
  CODE_EXECUTION_CONNECT_TIMEOUT: 10
  CODE_EXECUTION_READ_TIMEOUT: 60
  CODE_EXECUTION_WRITE_TIMEOUT: 10
  TEMPLATE_TRANSFORM_MAX_LENGTH: 80000
  WORKFLOW_MAX_EXECUTION_STEPS: 500
  WORKFLOW_MAX_EXECUTION_TIME: 1200
  WORKFLOW_CALL_MAX_DEPTH: 5
  MAX_VARIABLE_SIZE: 204800
  WORKFLOW_PARALLEL_DEPTH_LIMIT: 3
  WORKFLOW_FILE_UPLOAD_LIMIT: 10
  HTTP_REQUEST_NODE_MAX_BINARY_SIZE: 10485760
  HTTP_REQUEST_NODE_MAX_TEXT_SIZE: 1048576
  HTTP_REQUEST_NODE_SSL_VERIFY: True
  SSRF_PROXY_HTTP_URL: 'http://ssrf_proxy:3128'
  SSRF_PROXY_HTTPS_URL: 'http://ssrf_proxy:3128'
  LOOP_NODE_MAX_COUNT: 100
  MAX_TOOLS_NUM: 10
  MAX_PARALLEL_LIMIT: 10
  TEXT_GENERATION_TIMEOUT_MS: 60000
  PGUSER: ''
  POSTGRES_PASSWORD: ''
  POSTGRES_DB: ''
  PGDATA: /var/lib/postgresql/data/pgdata
  SANDBOX_API_KEY: dify-sandbox
  SANDBOX_GIN_MODE: release
  SANDBOX_WORKER_TIMEOUT: 15
  SANDBOX_ENABLE_NETWORK: true
  SANDBOX_HTTP_PROXY: 'http://ssrf_proxy:3128'
  SANDBOX_HTTPS_PROXY: 'http://ssrf_proxy:3128'
  SANDBOX_PORT: 8194
  WEAVIATE_PERSISTENCE_DATA_PATH: /var/lib/weaviate
  WEAVIATE_QUERY_DEFAULTS_LIMIT: 25
  WEAVIATE_AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: true
  WEAVIATE_DEFAULT_VECTORIZER_MODULE: none
  WEAVIATE_CLUSTER_HOSTNAME: node1
  WEAVIATE_AUTHENTICATION_APIKEY_ENABLED: true
  WEAVIATE_AUTHENTICATION_APIKEY_ALLOWED_KEYS: WVF5YThaHlkYwhGUSmCRgsX3tD5ngdN8pkih
  WEAVIATE_AUTHENTICATION_APIKEY_USERS: hello@dify.ai
  WEAVIATE_AUTHORIZATION_ADMINLIST_ENABLED: true
  WEAVIATE_AUTHORIZATION_ADMINLIST_USERS: hello@dify.ai
  CHROMA_SERVER_AUTHN_CREDENTIALS: difyai123456
  CHROMA_SERVER_AUTHN_PROVIDER: chromadb.auth.token_authn.TokenAuthenticationServerProvider
  CHROMA_IS_PERSISTENT: TRUE
  ORACLE_PWD: Dify123456
  ORACLE_CHARACTERSET: AL32UTF8
  ETCD_AUTO_COMPACTION_MODE: revision
  ETCD_AUTO_COMPACTION_RETENTION: 1000
  ETCD_QUOTA_BACKEND_BYTES: 4294967296
  ETCD_SNAPSHOT_COUNT: 50000
  MINIO_ACCESS_KEY: minioadmin
  MINIO_SECRET_KEY: minioadmin
  ETCD_ENDPOINTS: 'etcd:2379'
  MINIO_ADDRESS: 'minio:9000'
  MILVUS_AUTHORIZATION_ENABLED: true
  PGVECTOR_PGUSER: postgres
  PGVECTOR_POSTGRES_PASSWORD: difyai123456
  PGVECTOR_POSTGRES_DB: dify
  PGVECTOR_PGDATA: /var/lib/postgresql/data/pgdata
  OPENSEARCH_DISCOVERY_TYPE: single-node
  OPENSEARCH_BOOTSTRAP_MEMORY_LOCK: true
  OPENSEARCH_JAVA_OPTS_MIN: 512m
  OPENSEARCH_JAVA_OPTS_MAX: 1024m
  OPENSEARCH_INITIAL_ADMIN_PASSWORD: Qazwsxedc!@#123
  OPENSEARCH_MEMLOCK_SOFT: -1
  OPENSEARCH_MEMLOCK_HARD: -1
  OPENSEARCH_NOFILE_SOFT: 65536
  OPENSEARCH_NOFILE_HARD: 65536
  NGINX_SERVER_NAME: _
  NGINX_HTTPS_ENABLED: false
  NGINX_PORT: 80
  NGINX_SSL_PORT: 443
  NGINX_SSL_CERT_FILENAME: dify.crt
  NGINX_SSL_CERT_KEY_FILENAME: dify.key
  NGINX_SSL_PROTOCOLS: 'TLSv1.1 TLSv1.2 TLSv1.3'
  NGINX_WORKER_PROCESSES: auto
  NGINX_CLIENT_MAX_BODY_SIZE: 15M
  NGINX_KEEPALIVE_TIMEOUT: 65
  NGINX_PROXY_READ_TIMEOUT: 3600s
  NGINX_PROXY_SEND_TIMEOUT: 3600s
  NGINX_ENABLE_CERTBOT_CHALLENGE: false
  CERTBOT_EMAIL: your_email@example.com
  CERTBOT_DOMAIN: your_domain.com
  CERTBOT_OPTIONS: ''
  SSRF_HTTP_PORT: 3128
  SSRF_COREDUMP_DIR: /var/spool/squid
  SSRF_REVERSE_PROXY_PORT: 8194
  SSRF_SANDBOX_HOST: sandbox
  SSRF_DEFAULT_TIME_OUT: 5
  SSRF_DEFAULT_CONNECT_TIME_OUT: 5
  SSRF_DEFAULT_READ_TIME_OUT: 5
  SSRF_DEFAULT_WRITE_TIME_OUT: 5
  EXPOSE_NGINX_PORT: 80
  EXPOSE_NGINX_SSL_PORT: 443
  POSITION_TOOL_PINS: ''
  POSITION_TOOL_INCLUDES: ''
  POSITION_TOOL_EXCLUDES: ''
  POSITION_PROVIDER_PINS: ''
  POSITION_PROVIDER_INCLUDES: ''
  POSITION_PROVIDER_EXCLUDES: ''
  CSP_WHITELIST: ''
  CREATE_TIDB_SERVICE_JOB_ENABLED: false
  MAX_SUBMIT_COUNT: 100
  TOP_K_MAX_VALUE: 10
  DB_PLUGIN_DATABASE: dify_plugin
  EXPOSE_PLUGIN_DAEMON_PORT: 5002
  PLUGIN_DAEMON_PORT: 5002
  PLUGIN_DAEMON_KEY: lYkiYYT6owG+71oLerGzA7GXCgOT++6ovaezWAjpCjf+Sjc3ZtU+qUEi
  PLUGIN_DAEMON_URL: 'http://plugin_daemon:5002'
  PLUGIN_MAX_PACKAGE_SIZE: 52428800
  PLUGIN_PPROF_ENABLED: false
  PLUGIN_DEBUGGING_HOST: 0.0.0.0
  PLUGIN_DEBUGGING_PORT: 5003
  EXPOSE_PLUGIN_DEBUGGING_HOST: localhost
  EXPOSE_PLUGIN_DEBUGGING_PORT: 5003
  PLUGIN_DIFY_INNER_API_KEY: QaHbTe77CtuXmsfyhR7+vRjI/+XbV1AaFy691iy+kGDv2Jvy0/eAh8Y1
  PLUGIN_DIFY_INNER_API_URL: 'http://api:5001'
  ENDPOINT_URL_TEMPLATE: 'http://localhost/e/{hook_id}'
  MARKETPLACE_ENABLED: true
  MARKETPLACE_API_URL: 'https://marketplace.dify.ai'
  FORCE_VERIFYING_SIGNATURE: true
  PLUGIN_PYTHON_ENV_INIT_TIMEOUT: 120
  PLUGIN_MAX_EXECUTION_TIMEOUT: 600
  PIP_MIRROR_URL: ''

services:
  # API service
  api:
    image: langgenius/dify-api:1.1.3
    restart: always
    environment:
      # Use the shared environment variables.
      <<: *shared-api-worker-env
      # Startup mode, 'api' starts the API server.
      MODE: api
      SENTRY_DSN: ''
      SENTRY_TRACES_SAMPLE_RATE: 1.0
      SENTRY_PROFILES_SAMPLE_RATE: 1.0
      PLUGIN_REMOTE_INSTALL_HOST: localhost
      PLUGIN_REMOTE_INSTALL_PORT: 5003
      PLUGIN_MAX_PACKAGE_SIZE: 52428800
      INNER_API_KEY_FOR_PLUGIN: QaHbTe77CtuXmsfyhR7+vRjI/+XbV1AaFy691iy+kGDv2Jvy0/eAh8Y1
    depends_on:
      - db
      - redis
    volumes:
      # Mount the storage directory to the container, for storing user files.
      - ./volumes/app/storage:/app/api/storage
    networks:
      - ssrf_proxy_network
      - default

  # worker service
  # The Celery worker for processing the queue.
  worker:
    image: langgenius/dify-api:1.1.3
    restart: always
    environment:
      # Use the shared environment variables.
      <<: *shared-api-worker-env
      # Startup mode, 'worker' starts the Celery worker for processing the queue.
      MODE: worker
      SENTRY_DSN: ''
      SENTRY_TRACES_SAMPLE_RATE: 1.0
      SENTRY_PROFILES_SAMPLE_RATE: 1.0
      PLUGIN_MAX_PACKAGE_SIZE: 52428800
      INNER_API_KEY_FOR_PLUGIN: QaHbTe77CtuXmsfyhR7+vRjI/+XbV1AaFy691iy+kGDv2Jvy0/eAh8Y1
    depends_on:
      - db
      - redis
    volumes:
      # Mount the storage directory to the container, for storing user files.
      - ./volumes/app/storage:/app/api/storage
    networks:
      - ssrf_proxy_network
      - default

  # Frontend web application.
  web:
    image: langgenius/dify-web:1.1.3
    restart: always
    environment:
      CONSOLE_API_URL: ''
      APP_API_URL: ''
      SENTRY_DSN: ''
      NEXT_TELEMETRY_DISABLED: 0
      TEXT_GENERATION_TIMEOUT_MS: 60000
      CSP_WHITELIST: ''
      MARKETPLACE_API_URL: 'https://marketplace.dify.ai'
      MARKETPLACE_URL: 'https://marketplace.dify.ai'
      TOP_K_MAX_VALUE: 10
      INDEXING_MAX_SEGMENTATION_TOKENS_LENGTH: 4000
      PM2_INSTANCES: 2
      LOOP_NODE_MAX_COUNT: 100
      MAX_TOOLS_NUM: 10
      MAX_PARALLEL_LIMIT: 10

  # The postgres database.
  db:
    image: postgres:15-alpine
    restart: always
    environment:
      PGUSER: postgres
      POSTGRES_PASSWORD: difyai123456
      POSTGRES_DB: dify
      PGDATA: /var/lib/postgresql/data/pgdata
    command: >
      postgres -c 'max_connections=100'
               -c 'shared_buffers=128MB'
               -c 'work_mem=4MB'
               -c 'maintenance_work_mem=64MB'
               -c 'effective_cache_size=4096MB'
    volumes:
      - ./volumes/db/data:/var/lib/postgresql/data
    healthcheck:
      test: [ 'CMD', 'pg_isready' ]
      interval: 1s
      timeout: 3s
      retries: 30

  # The redis cache.
  redis:
    image: redis:6-alpine
    restart: always
    environment:
      REDISCLI_AUTH: difyai123456
    volumes:
      # Mount the redis data directory to the container.
      - ./volumes/redis/data:/data
    # Set the redis password when startup redis server.
    command: redis-server --requirepass difyai123456
    healthcheck:
      test: [ 'CMD', 'redis-cli', 'ping' ]

  # The DifySandbox
  sandbox:
    image: langgenius/dify-sandbox:0.2.11
    restart: always
    environment:
      # The DifySandbox configurations
      # Make sure you are changing this key for your deployment with a strong key.
      # You can generate a strong key using `openssl rand -base64 42`.
      API_KEY: dify-sandbox
      GIN_MODE: release
      WORKER_TIMEOUT: 15
      ENABLE_NETWORK: true
      HTTP_PROXY: 'http://ssrf_proxy:3128'
      HTTPS_PROXY: 'http://ssrf_proxy:3128'
      SANDBOX_PORT: 8194
    volumes:
      - ./volumes/sandbox/dependencies:/dependencies
      - ./volumes/sandbox/conf:/conf
    healthcheck:
      test: [ 'CMD', 'curl', '-f', 'http://localhost:8194/health' ]
    networks:
      - ssrf_proxy_network

  # plugin daemon
  plugin_daemon:
    image: langgenius/dify-plugin-daemon:0.0.6-local
    restart: always
    environment:
      # Use the shared environment variables.
      <<: *shared-api-worker-env
      DB_DATABASE: dify_plugin
      SERVER_PORT: 5002
      SERVER_KEY: lYkiYYT6owG+71oLerGzA7GXCgOT++6ovaezWAjpCjf+Sjc3ZtU+qUEi
      MAX_PLUGIN_PACKAGE_SIZE: 52428800
      PPROF_ENABLED: false
      DIFY_INNER_API_URL: 'http://api:5001'
      DIFY_INNER_API_KEY: QaHbTe77CtuXmsfyhR7+vRjI/+XbV1AaFy691iy+kGDv2Jvy0/eAh8Y1
      PLUGIN_REMOTE_INSTALLING_HOST: 0.0.0.0
      PLUGIN_REMOTE_INSTALLING_PORT: 5003
      PLUGIN_WORKING_PATH: /app/storage/cwd
      FORCE_VERIFYING_SIGNATURE: true
      PYTHON_ENV_INIT_TIMEOUT: 120
      PLUGIN_MAX_EXECUTION_TIMEOUT: 600
      PIP_MIRROR_URL: ''
    ports:
      - "5003:5003"
    volumes:
      - ./volumes/plugin_daemon:/app/storage
    depends_on:
      - db

  # ssrf_proxy server
  # for more information, please refer to
  # https://docs.dify.ai/learn-more/faq/install-faq#id-18.-why-is-ssrf_proxy-needed
  ssrf_proxy:
    image: ubuntu/squid:latest
    restart: always
    volumes:
      - ./ssrf_proxy/squid.conf.template:/etc/squid/squid.conf.template
      - ./ssrf_proxy/docker-entrypoint.sh:/docker-entrypoint-mount.sh
    entrypoint: [ 'sh', '-c', "cp /docker-entrypoint-mount.sh /docker-entrypoint.sh && sed -i 's/\r$$//' /docker-entrypoint.sh && chmod +x /docker-entrypoint.sh && /docker-entrypoint.sh" ]
    environment:
      # pls clearly modify the squid env vars to fit your network environment.
      HTTP_PORT: 3128
      COREDUMP_DIR: /var/spool/squid
      REVERSE_PROXY_PORT: 8194
      SANDBOX_HOST: sandbox
      SANDBOX_PORT: 8194
    networks:
      - ssrf_proxy_network
      - default

  # Certbot service
  # use `docker-compose --profile certbot up` to start the certbot service.
  certbot:
    image: certbot/certbot
    profiles:
      - certbot
    volumes:
      - ./volumes/certbot/conf:/etc/letsencrypt
      - ./volumes/certbot/www:/var/www/html
      - ./volumes/certbot/logs:/var/log/letsencrypt
      - ./volumes/certbot/conf/live:/etc/letsencrypt/live
      - ./certbot/update-cert.template.txt:/update-cert.template.txt
      - ./certbot/docker-entrypoint.sh:/docker-entrypoint.sh
    environment:
      - CERTBOT_EMAIL=your_email@example.com
      - CERTBOT_DOMAIN=your_domain.com
      - CERTBOT_OPTIONS=''
    entrypoint: [ '/docker-entrypoint.sh' ]
    command: [ 'tail', '-f', '/dev/null' ]

  # The nginx reverse proxy.
  # used for reverse proxying the API service and Web service.
  nginx:
    image: nginx:latest
    restart: always
    volumes:
      - ./nginx/nginx.conf.template:/etc/nginx/nginx.conf.template
      - ./nginx/proxy.conf.template:/etc/nginx/proxy.conf.template
      - ./nginx/https.conf.template:/etc/nginx/https.conf.template
      - ./nginx/conf.d:/etc/nginx/conf.d
      - ./nginx/docker-entrypoint.sh:/docker-entrypoint-mount.sh
      - ./nginx/ssl:/etc/ssl # cert dir (legacy)
      - ./volumes/certbot/conf/live:/etc/letsencrypt/live # cert dir (with certbot container)
      - ./volumes/certbot/conf:/etc/letsencrypt
      - ./volumes/certbot/www:/var/www/html
    entrypoint: [ 'sh', '-c', "cp /docker-entrypoint-mount.sh /docker-entrypoint.sh && sed -i 's/\r$$//' /docker-entrypoint.sh && chmod +x /docker-entrypoint.sh && /docker-entrypoint.sh" ]
    environment:
      NGINX_SERVER_NAME: _
      NGINX_HTTPS_ENABLED: false
      NGINX_SSL_PORT: 443
      NGINX_PORT: 80
      # You're required to add your own SSL certificates/keys to the `./nginx/ssl` directory
      # and modify the env vars below in .env if HTTPS_ENABLED is true.
      NGINX_SSL_CERT_FILENAME: dify.crt
      NGINX_SSL_CERT_KEY_FILENAME: dify.key
      NGINX_SSL_PROTOCOLS: 'TLSv1.1 TLSv1.2 TLSv1.3'
      NGINX_WORKER_PROCESSES: auto
      NGINX_CLIENT_MAX_BODY_SIZE: 15M
      NGINX_KEEPALIVE_TIMEOUT: 65
      NGINX_PROXY_READ_TIMEOUT: 3600s
      NGINX_PROXY_SEND_TIMEOUT: 3600s
      NGINX_ENABLE_CERTBOT_CHALLENGE: false
      CERTBOT_DOMAIN: your_domain.com
    depends_on:
      - api
      - web
    ports:
      - '80:80'
      - '443:443'

  # The Weaviate vector store.
  weaviate:
    image: semitechnologies/weaviate:1.19.0
    profiles:
      - ''
      - weaviate
    restart: always
    volumes:
      # Mount the Weaviate data directory to the con tainer.
      - ./volumes/weaviate:/var/lib/weaviate
    environment:
      # The Weaviate configurations
      # You can refer to the [Weaviate](https://weaviate.io/developers/weaviate/config-refs/env-vars) documentation for more information.
      PERSISTENCE_DATA_PATH: /var/lib/weaviate
      QUERY_DEFAULTS_LIMIT: 25
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: true
      DEFAULT_VECTORIZER_MODULE: none
      CLUSTER_HOSTNAME: node1
      AUTHENTICATION_APIKEY_ENABLED: true
      AUTHENTICATION_APIKEY_ALLOWED_KEYS: WVF5YThaHlkYwhGUSmCRgsX3tD5ngdN8pkih
      AUTHENTICATION_APIKEY_USERS: hello@dify.ai
      AUTHORIZATION_ADMINLIST_ENABLED: true
      AUTHORIZATION_ADMINLIST_USERS: hello@dify.ai

  # Qdrant vector store.
  # (if used, you need to set VECTOR_STORE to qdrant in the api & worker service.)
  qdrant:
    image: langgenius/qdrant:v1.7.3
    profiles:
      - qdrant
    restart: always
    volumes:
      - ./volumes/qdrant:/qdrant/storage
    environment:
      QDRANT_API_KEY: difyai123456

  # The Couchbase vector store.
  couchbase-server:
    build: ./couchbase-server
    profiles:
      - couchbase
    restart: always
    environment:
      - CLUSTER_NAME=dify_search
      - COUCHBASE_ADMINISTRATOR_USERNAME=Administrator
      - COUCHBASE_ADMINISTRATOR_PASSWORD=password
      - COUCHBASE_BUCKET=Embeddings
      - COUCHBASE_BUCKET_RAMSIZE=512
      - COUCHBASE_RAM_SIZE=2048
      - COUCHBASE_EVENTING_RAM_SIZE=512
      - COUCHBASE_INDEX_RAM_SIZE=512
      - COUCHBASE_FTS_RAM_SIZE=1024
    hostname: couchbase-server
    container_name: couchbase-server
    working_dir: /opt/couchbase
    stdin_open: true
    tty: true
    entrypoint: [ "" ]
    command: sh -c "/opt/couchbase/init/init-cbserver.sh"
    volumes:
      - ./volumes/couchbase/data:/opt/couchbase/var/lib/couchbase/data
    healthcheck:
      # ensure bucket was created before proceeding
      test: [ "CMD-SHELL", "curl -s -f -u Administrator:password http://localhost:8091/pools/default/buckets | grep -q '\\[{' || exit 1" ]
      interval: 10s
      retries: 10
      start_period: 30s
      timeout: 10s

  # The pgvector vector database.
  pgvector:
    image: pgvector/pgvector:pg16
    profiles:
      - pgvector
    restart: always
    environment:
      PGUSER: postgres
      # The password for the default postgres user.
      POSTGRES_PASSWORD: difyai123456
      # The name of the default postgres database.
      POSTGRES_DB: dify
      # postgres data directory
      PGDATA: /var/lib/postgresql/data/pgdata
      # pg_bigm module for full text search
      PG_BIGM: false
      PG_BIGM_VERSION: 1.2-20240606
    volumes:
      - ./volumes/pgvector/data:/var/lib/postgresql/data
      - ./pgvector/docker-entrypoint.sh:/docker-entrypoint.sh
    entrypoint: [ '/docker-entrypoint.sh' ]
    healthcheck:
      test: [ 'CMD', 'pg_isready' ]
      interval: 1s
      timeout: 3s
      retries: 30

  # pgvecto-rs vector store
  pgvecto-rs:
    image: tensorchord/pgvecto-rs:pg16-v0.3.0
    profiles:
      - pgvecto-rs
    restart: always
    environment:
      PGUSER: postgres
      # The password for the default postgres user.
      POSTGRES_PASSWORD: difyai123456
      # The name of the default postgres database.
      POSTGRES_DB: dify
      # postgres data directory
      PGDATA: /var/lib/postgresql/data/pgdata
    volumes:
      - ./volumes/pgvecto_rs/data:/var/lib/postgresql/data
    healthcheck:
      test: [ 'CMD', 'pg_isready' ]
      interval: 1s
      timeout: 3s
      retries: 30

  # Chroma vector database
  chroma:
    image: ghcr.io/chroma-core/chroma:0.5.20
    profiles:
      - chroma
    restart: always
    volumes:
      - ./volumes/chroma:/chroma/chroma
    environment:
      CHROMA_SERVER_AUTHN_CREDENTIALS: difyai123456
      CHROMA_SERVER_AUTHN_PROVIDER: chromadb.auth.token_authn.TokenAuthenticationServerProvider
      IS_PERSISTENT: TRUE

  # OceanBase vector database
  oceanbase:
    image: quay.io/oceanbase/oceanbase-ce:4.3.3.0-100000142024101215
    profiles:
      - oceanbase
    restart: always
    volumes:
      - ./volumes/oceanbase/data:/root/ob
      - ./volumes/oceanbase/conf:/root/.obd/cluster
      - ./volumes/oceanbase/init.d:/root/boot/init.d
    environment:
      OB_MEMORY_LIMIT: 6G
      OB_SYS_PASSWORD: difyai123456
      OB_TENANT_PASSWORD: difyai123456
      OB_CLUSTER_NAME: difyai
      OB_SERVER_IP: '127.0.0.1'

  # Oracle vector database
  oracle:
    image: container-registry.oracle.com/database/free:latest
    profiles:
      - oracle
    restart: always
    volumes:
      - source: oradata
        type: volume
        target: /opt/oracle/oradata
      - ./startupscripts:/opt/oracle/scripts/startup
    environment:
      ORACLE_PWD: Dify123456
      ORACLE_CHARACTERSET: AL32UTF8

  # Milvus vector database services
  etcd:
    container_name: milvus-etcd
    image: quay.io/coreos/etcd:v3.5.5
    profiles:
      - milvus
    environment:
      ETCD_AUTO_COMPACTION_MODE: revision
      ETCD_AUTO_COMPACTION_RETENTION: 1000
      ETCD_QUOTA_BACKEND_BYTES: 4294967296
      ETCD_SNAPSHOT_COUNT: 50000
    volumes:
      - ./volumes/milvus/etcd:/etcd
    command: etcd -advertise-client-urls=http://127.0.0.1:2379 -listen-client-urls http://0.0.0.0:2379 --data-dir /etcd
    healthcheck:
      test: [ 'CMD', 'etcdctl', 'endpoint', 'health' ]
      interval: 30s
      timeout: 20s
      retries: 3
    networks:
      - milvus

  minio:
    container_name: milvus-minio
    image: minio/minio:RELEASE.2023-03-20T20-16-18Z
    profiles:
      - milvus
    environment:
      MINIO_ACCESS_KEY: minioadmin
      MINIO_SECRET_KEY: minioadmin
    volumes:
      - ./volumes/milvus/minio:/minio_data
    command: minio server /minio_data --console-address ":9001"
    healthcheck:
      test: [ 'CMD', 'curl', '-f', 'http://localhost:9000/minio/health/live' ]
      interval: 30s
      timeout: 20s
      retries: 3
    networks:
      - milvus

  milvus-standalone:
    container_name: milvus-standalone
    image: milvusdb/milvus:v2.5.0-beta
    profiles:
      - milvus
    command: [ 'milvus', 'run', 'standalone' ]
    environment:
      ETCD_ENDPOINTS: 'etcd:2379'
      MINIO_ADDRESS: 'minio:9000'
      common.security.authorizationEnabled: true
    volumes:
      - ./volumes/milvus/milvus:/var/lib/milvus
    healthcheck:
      test: [ 'CMD', 'curl', '-f', 'http://localhost:9091/healthz' ]
      interval: 30s
      start_period: 90s
      timeout: 20s
      retries: 3
    depends_on:
      - etcd
      - minio
    ports:
      - 19530:19530
      - 9091:9091
    networks:
      - milvus

  # Opensearch vector database
  opensearch:
    container_name: opensearch
    image: opensearchproject/opensearch:latest
    profiles:
      - opensearch
    environment:
      discovery.type: single-node
      bootstrap.memory_lock: true
      OPENSEARCH_JAVA_OPTS: -Xms512m -Xmx1024m
      OPENSEARCH_INITIAL_ADMIN_PASSWORD: Qazwsxedc!@#123
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    volumes:
      - ./volumes/opensearch/data:/usr/share/opensearch/data
    networks:
      - opensearch-net

  opensearch-dashboards:
    container_name: opensearch-dashboards
    image: opensearchproject/opensearch-dashboards:latest
    profiles:
      - opensearch
    environment:
      OPENSEARCH_HOSTS: '["https://opensearch:9200"]'
    volumes:
      - ./volumes/opensearch/opensearch_dashboards.yml:/usr/share/opensearch-dashboards/config/opensearch_dashboards.yml
    networks:
      - opensearch-net
    depends_on:
      - opensearch

  # opengauss vector database.
  opengauss:
    image: opengauss/opengauss:7.0.0-RC1
    profiles:
      - opengauss
    privileged: true
    restart: always
    environment:
      GS_USERNAME: postgres
      GS_PASSWORD: Dify@123
      GS_PORT: 6600
      GS_DB: dify
    volumes:
      - ./volumes/opengauss/data:/var/lib/opengauss/data
    healthcheck:
      test: ["CMD-SHELL", "netstat -lntp | grep tcp6 > /dev/null 2>&1"]
      interval: 10s
      timeout: 10s
      retries: 10
    ports:
      - 6600:6600

  # MyScale vector database
  myscale:
    container_name: myscale
    image: myscale/myscaledb:1.6.4
    profiles:
      - myscale
    restart: always
    tty: true
    volumes:
      - ./volumes/myscale/data:/var/lib/clickhouse
      - ./volumes/myscale/log:/var/log/clickhouse-server
      - ./volumes/myscale/config/users.d/custom_users_config.xml:/etc/clickhouse-server/users.d/custom_users_config.xml
    ports:
      - 8123:8123

  # https://www.elastic.co/guide/en/elasticsearch/reference/current/settings.html
  # https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html#docker-prod-prerequisites
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.14.3
    container_name: elasticsearch
    profiles:
      - elasticsearch
      - elasticsearch-ja
    restart: always
    volumes:
      - ./elasticsearch/docker-entrypoint.sh:/docker-entrypoint-mount.sh
      - dify_es01_data:/usr/share/elasticsearch/data
    environment:
      ELASTIC_PASSWORD: elastic
      VECTOR_STORE: weaviate
      cluster.name: dify-es-cluster
      node.name: dify-es0
      discovery.type: single-node
      xpack.license.self_generated.type: basic
      xpack.security.enabled: 'true'
      xpack.security.enrollment.enabled: 'false'
      xpack.security.http.ssl.enabled: 'false'
    ports:
      - 9200:9200
    deploy:
      resources:
        limits:
          memory: 2g
    entrypoint: [ 'sh', '-c', "sh /docker-entrypoint-mount.sh" ]
    healthcheck:
      test: [ 'CMD', 'curl', '-s', 'http://localhost:9200/_cluster/health?pretty' ]
      interval: 30s
      timeout: 10s
      retries: 50

  # https://www.elastic.co/guide/en/kibana/current/docker.html
  # https://www.elastic.co/guide/en/kibana/current/settings.html
  kibana:
    image: docker.elastic.co/kibana/kibana:8.14.3
    container_name: kibana
    profiles:
      - elasticsearch
    depends_on:
      - elasticsearch
    restart: always
    environment:
      XPACK_ENCRYPTEDSAVEDOBJECTS_ENCRYPTIONKEY: d1a66dfd-c4d3-4a0a-8290-2abcb83ab3aa
      NO_PROXY: localhost,127.0.0.1,elasticsearch,kibana
      XPACK_SECURITY_ENABLED: 'true'
      XPACK_SECURITY_ENROLLMENT_ENABLED: 'false'
      XPACK_SECURITY_HTTP_SSL_ENABLED: 'false'
      XPACK_FLEET_ISAIRGAPPED: 'true'
      I18N_LOCALE: zh-CN
      SERVER_PORT: '5601'
      ELASTICSEARCH_HOSTS: http://elasticsearch:9200
    ports:
      - 5601:5601
    healthcheck:
      test: [ 'CMD-SHELL', 'curl -s http://localhost:5601 >/dev/null || exit 1' ]
      interval: 30s
      timeout: 10s
      retries: 3

  # unstructured .
  # (if used, you need to set ETL_TYPE to Unstructured in the api & worker service.)
  unstructured:
    image: downloads.unstructured.io/unstructured-io/unstructured-api:latest
    profiles:
      - unstructured
    restart: always
    volumes:
      - ./volumes/unstructured:/app/data

networks:
  # create a network between sandbox, api and ssrf_proxy, and can not access outside.
  ssrf_proxy_network:
    driver: bridge
    internal: true
  milvus:
    driver: bridge
  opensearch-net:
    driver: bridge
    internal: true

volumes:
  oradata:
  dify_es01_data:
